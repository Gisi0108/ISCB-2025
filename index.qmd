---
title: "ISCB2025 Concepts"
---

# GEE based methods

## Mathematical backgroud

The generalized estimation equations have the following assumptions:

-   Independent measurements across the clusters (individuals)
-   Measurements might correlate within clusters

Details on the following concept can be found in [@RN19].

The response variables for the $i^{\text{th}}$ subject can be grouped into an $n_i \times 1$ vector $$
Y_i = \begin{pmatrix}
  Y_{i1} \\ \vdots \\ Y_{in_i}
  \end{pmatrix}, \
  i = 1, \ldots , N
$$ (for simplicity in longitudinal cohort data: $n_i = T$ for all $i$). Associated with each response, $Y_{ij}$ there is a $p \times 1$ vector of covariates $$
X_{ij} = \begin{pmatrix}
  X_{ij1} \\ \vdots \\ X_{ijp}
  \end{pmatrix}, \
  j = 1, \ldots , T,
$$ which can be grouped to $$
X_i = \begin{pmatrix}
  X_{i1}^T \\ \vdots \\ X_{iT}^T
  \end{pmatrix}, 
  = \begin{pmatrix}
  X_{i11} & \dots & X_{i1p} \\
  \vdots & \ddots & \vdots \\
  X_{iT1} & \dots & X_{iTp}
  \end{pmatrix}.
$$

The generalized *(marginal) mean model* for coefficients $\beta = (\beta_1, \ldots, \beta_p)^T$ looks like this $$
\begin{split}
\mathbb{E} \left[ Y_{ij} \mid X_{i} \right] = \mathbb{E} \left[ Y_{ij} \mid X_{ij} \right] &= \mu_{ij} \\
g(\mu_{ij}) &= \beta_0 + \beta_1 X_{ij1} + \ldots + \beta_p X_{ijp} \\
&= X_{ij}^T \beta
\end{split}
$$

(with a little abuse of the notation, ignoring the intercept $\beta_0$).

We assume, that there exists a known 'variance function' $v$ and a corresponding scale parameter $\phi$ such that $$
V_{ij} := \text{Var} \left[ Y_{ij} \mid X_{i} \right] = \phi v(\mu_{ij}).
$$

To adjust for the intra-class correlation structure (e.g. within the individuals by repeated measurements), we need more flexibility of the covariance. By setting $A_i := \text{diag}(V_{ij})$ and further $$
\begin{split}
\text{Corr}\left[ Y_{ij}, Y_{ik} \mid X_{i} \right] &= \varrho_{ijk}(\alpha) \\
\text{Corr} \left[Y_i \mid X_i \right] = R_i(\alpha) &= (\varrho_{ijk}(\alpha))_{jk} :=  
\begin{pmatrix}
         1 & \alpha_{12} & \ldots & \alpha_{1p} \\
         \alpha_{21} & 1 & \ldots & \alpha_{2p} \\
         \vdots & \vdots & \ddots & \vdots \\
         \alpha_{p1} & \alpha_{p2} & \dots & 1
    \end{pmatrix}, \\
\end{split}
$$

we can define the *working covariance matrix* $$
 \text{Cov} \left[Y_i \mid X_i \right] = V_i(\alpha) := A_i^{1/2} R_i(\alpha) A_i^{1/2}.
$$ The parameters $\alpha$ characterizes the correlation.

Note: Since $\mu_{ij} = \mu_{ij}(\beta)$ is dependent on the regression coefficients, we also have $V_i(\alpha) = V_i(\alpha, \beta)$.

To estimate the regression coefficients $\beta$, we have to solve the following score function: $$
\begin{split}
U(\beta, \alpha) = U(\beta) :=& \sum_{i=1}^N D_i^T V_i^{-1}(Y_i-\mu_i) \\
\overset{!}{=}& \ 0,
\end{split}
$$ {#eq-gee}

with $D_i := \frac{\partial \mu_i}{\partial \beta}$.

Note: In the Gaussian case, i.e. $g(\mu_{ij}) = \mu_{ij}$, we have $\frac{\partial \mu_i}{\partial \beta} = X_i$. Additional, when writing $g(\mu_{ij}) =: \eta_{ij}$ in the general case we get $\frac{\partial \mu_i}{\partial \beta} = \frac{\partial \mu_i}{\partial \eta_{i}} X_i$

## Identified software packages

The following R packages are based on the GEE framework.

### PGEE \[Grey list\]

**PGEE** does not provide a default threshold to decide if a variable is selected or not. Therefore it was put on the gray list. But since most of the following GEE-based packages relies on this method, it is presented at the beginning of this section:

The R package **PGEE** [@RN16] adds a penalty term to the [score equation @eq-gee]. In more detail, the SCAD penalty is chosen: $$
q_{\lambda}(\theta) := \lambda \left( 
  1_{\theta \leq \lambda} + 
  \frac{(a \lambda - \theta)_+}{(a-1)\lambda} 1_{\theta > \lambda} \right)
$$ Where $\theta \geq 0$, $a > 2$ (set to $a=3.7$). Note: In (ref), there is a dependency of $p$ on $N$, but in our use, we choose $N$ fix. The penalized regression coefficients can be calculated by solving the modified equation: $$
\begin{split}
U^\text{PGEE}(\beta) :=& \ U(\beta) - q_\lambda(|\beta|) \circ \text{sign}(\beta)  \\
\overset{!}{=}& \ 0,
\end{split}
$$ {#eq-pgee} where $\circ$ denotes the Hadamard product (element-wise product) of two vectors.

### geeVerse

The R package **geeVerse** [@RN14] is based on **PGEE** but uses a quantile regression approach, e.g. for given quantile $\tau \in (0,1)$ we consider $$
\theta_\tau(Y_i \mid X_i) = X_i \beta_\tau
$$ with different regression coefficients $\beta_\tau (=: \beta)$ at different quantile levels. Instead of using $U(\beta)$, we have to adjust this a bit: $$
\tilde{U}(\beta) :=  \sum_{i=1}^N X_i^T \Gamma_i R_i^{-1}(\tau - 1_{Y_i \leq X_i\beta}),
$$ where $\Gamma_i$ is a $T \times T$ diagonal matrix of the conditional density of error $Y_{ij} - X_{ij}\beta$ and $\tau - 1_{Y_i \leq X_i\beta}$ is seen component wise.

The corresponding quantile penalized generalized estimating equation is then defined as $$
\begin{split}
U^\text{geeVerse}(\beta) :=& \ \tilde{U}(\beta) - q_\lambda(|\beta|) \circ \text{sign}(\beta)  \\
\overset{!}{=}& \ 0.
\end{split}
$$ {#eq-geeverse}

### sgee

The package **sgee** [@RN18] builds a model via a *stagewise* procedure: Start with a null model, i.e. $\beta^{[0]} := 0$ and update this to $\beta^{[t]} := \beta^{[t-1]} + \delta^{[t]}$, $t \in \mathbb{N}$. In the general framework, the update increment is defined as $$
\begin{split}
\delta^{[t]} :=& \ \underset{\delta \in \mathbb{R}^p}{\mathrm{arg\min}}
  f(\beta^{[t-1]} + \delta) - f(\beta^{[t-1]}) \\
  & \text{subject to} \ \phi(\beta^{[t-1]} + \delta) - \phi(\beta^{[t-1]}) \leq \varepsilon,
\end{split}
$$ where $\varepsilon > 0$ is the step size and $\phi$ the penalty function (which satisfies the triangular inequality). Via Taylor expansion, one can approximate the general framework to $$
\delta^{[t]} = \underset{\delta \in \mathbb{R}^p}{\mathrm{arg\min}}
 \langle \nabla f(\beta^{[t-1]}) , \delta \rangle \ \text{subject to} \ \phi(\delta) \leq \varepsilon.
$$

To merge the [GEE approach @eq-gee] to this structure, we interpret $U(\beta, \nu) = \nabla f(\beta, \nu)$, where the nuisance parameters $\nu = (\beta_0, \psi, \alpha)$ contains information about the intercept $\beta_0$, the dispersion $\psi$ and the intra-correlation structure $\alpha$. The stagewise procedure starts then also with $\beta^{[0]} = 0$ and updates to

$$
\begin{split}
i) & \ \text{Given} \ \beta^{[t-1]}, \ \text{update the nuisance parameters to obtain} \ \nu^{[t]} \\
ii) & \ \delta^{[t]} = \underset{\delta \in \mathbb{R}^p}{\mathrm{arg\min}}
      \langle U(\beta^{[t-1]}, \nu^{[t]}) , \delta \rangle \ 
      \text{subject to} \ \phi(\delta) \leq \varepsilon, \\
iii) & \ \beta^{[t]} = \beta^{[t-1]} + \delta^{[t]},
\end{split}
$$

where $\phi(\delta) = \lVert \delta \rVert_1$ is the LASSO penalty.

Note: In [@RN18], the procedure is generalized for the bi-level setting using the group LASSO (gLASSO) penalty.

### LassoGEE

No references found. But checking the code of the main functions reveals the direct usage of the **PGEE** functions, but uses own functions to include the $L^1$-penalty.

### pgee.mixed

Can be applied for multivariate outcomes and is for the univariate case the same as **PGEE**. There is an additional method for controlling the FDR of the selection, but this is (currently) only in the multivariate setting available.

# Mixed effects models based methods

## Mathematical backgroud

In contrast to the approach via a marginal model (GEE), the mixed effects models are based on the assumption of heterogeneity across the individuals in some predictor variables. Using the same notation as above, we introduce a sub-matrix $Z_i \leq X_i$, i.e. all columns of $Z_i$ are columns of $X_i$. Given a collection of additional coefficients $b_i \sim N(0,G)$ and some errors $\varepsilon_i \sim N(0,\sigma^2I_T)$ for each subject $i$ (all independent of the predictors $X$), we can write

$$
Y_i = X_i^T \beta + Z_i^T b_i + \varepsilon_i.
$$ The regression is now formulated as

$$
\mu_{ij} := E[Y_{ij} \mid b_i] = X_{ij}^T \beta + Z_{ij}^T b_i
$$

where $\beta$ are interpreted as *fixed effects* and $b_i$ as *random effects*.

The generalized mixed effects are therefore given by

$$
\eta_{ij} := g(E[Y_{ij} \mid b_i]) = X_{ij}^T \beta + Z_{ij}^T b_i.
$$ {#eq-glmm}

The term $X_{ij}^T\beta$ can be seen as difference from the population mean and the additional term $Z_{ij}^Tb_i$ accounts the subject-specific effect. Therefore, the $b_i$ introduces a correlation structure (marginally) between the $Y_i$.

Note: As usual, $(\mu_{ij})_j$ are independent and belong to the exponential family with $\text{Var} [\mu_{ij}] = v(\mu_{ij})\phi$ for a variance function $v$ and a dispersion parameter $\phi$.

A common procedure to solve the regression formula for mixed effects models is via maximum likelihood estimation. The basis assumptions implies that $Y_i | b_i$ has an exponential family distribution and $b_i$ has a multivariate normal distribution with zero mean and covariance matrix $G$. The (integrated) likelihood function is then defined as $$
L(\beta, \phi, G) = \prod_{i=1} ^N\int f(Y_i|b_i; \phi) f(b_i; G) \ \text{d}b_i.
$$ Therefore, the likelihood depends on the covariance of $b_i$, but not on the unobserved $b_i$. More details can be found in [@RN19].

As usual, the lowercase $\ell$ denotes the log-likelihood function. Since the above integral has no closed form, a numerical solution to the (log-)likelihood function is often computed via an approximation of the integrand. This is the so-called *penalized quasi-likelihood* (PQL): $$
\ell^{\text{PQL}}(\beta, \phi, G) := \sum_{i=1}^N \log\big(f(Y_i|b_i)\big) - \frac{1}{2}b^TGb.
$$

## Identified software packages

The following R packages are based on the GLMM framework.

### splmm

The idea of the approach of [@yang2022] is to penalize both fixed and random effects. To penalize the random effects, the correlation matrix must be considered: Rewrite $G = \sigma^2 D$ and apply the Cholesky decomposition $D = LL^T$. The advantage of this decomposition is that now the $k$-th row of $L$ ($L_{(k)}$) is linked to the $k$-th random effect, i.e. if $L_{(k)} = 0$, the $k$-th random effect will be removed from the model. A similar approximation is done to simplify the likelihood function and in addition, the nuisance $\phi$ is substituted leading to the *profile log-likelihood* $$
\ell^{\text{PL}}(\beta, D) := \frac{1}{2} \sum_{i=1}^N \log|V_i| + \frac{N}{2}\log\Big(\sum_{i=1}^N (Y_i - X_i\beta)^T V_i^{-1} (Y_i - X_i\beta) \Big),
$$ with $V_i := Z_iDZ_i^T + I_T$ the (standardized) conditional covariance matrix of $Y_i$. Extending this with two simultaneous applied penalties leads to the final objective function: $$
\ell^{\text{splmm}}(\beta, D) := \ell^{\text{PL}}(\beta, D) + \sum_{p'=1}^p P_{\lambda_1}(|\beta_{p'}|) + \sum_{q'=2}^q P_{\lambda_2}(\lVert L_{(k)} \rVert_2).
$$ The first penalty function, $P_{\lambda_1}$ regulates the sparsity of $\beta$, while $P_{\lambda_2}$ regulates the sparsity of the random effects (via sparsity of $L$). Starting at $q' = 2$ ensures to keep the random intercept in the model.

Note: $P_{\lambda_2}$ introduces a group penalty on the rows of $L$, achieving the shrinkage of all entries of a certain row to $0$.

The **splmm** package currently supports the LASSO and SCAD penalties as independent choices for $P_{\lambda_k}$.

### rpql

[@hui2017] built up a similar framework, by subtracting additional terms from the PQL: $$
\ell^{\text{rpql}}(\beta, \phi, G) := \ell^{\text{PQL}}(\beta, \phi, G) - \lambda\sum_{p' = 1}^pv_{p'}|\beta_{p'}| - \lambda \sum_{q'=1}^q w_{q'}\lVert b_{\bullet q'} \rVert_2,
$$ where $v_{p'}$ and $w_{q'}$ are weights analogous to the adaptive (group) LASSO (i.e. the reciprocal of the OLS) . The coefficients are then also estimated by $\text{argmax} \ \ell^{\text{rpql}}(\beta, \phi, G)$.

### buildmer

There is no peer-reviewed publication for this package, even though it is the most frequently used package in this review ($> 170,000$ all-time downloads). Only a [vignette](https://cran.r-project.org/web/packages/buildmer/vignettes/buildmer.html) is revealing details on this method. The core idea is 'finding the maximal *feasible* model & doing stepwise elimination from it'.

This algorithm is also available for the frameworks of Generalized Additive Models (GAM) and Mixed-Effects-Regression Trees (MERT).

### plsmmLasso

To understand this method, we first have to introduce the *Generalized Semiparametric Mixed Models* (GSMM) [@taavoni2021]: $$
\eta_{ij}^{\text{GSMM}} := X_{ij}^T \beta + Z_{ij}^T b_i + f(t_{ij}),
$$ where $t_{ij}$ is the time point of the $j$-th measurement of subject $i$ and $f$ a continuous and twice differentiable function on some finite interval. The unspecified function $f$ is approximate via $$
\begin{split}
f(t_{ij}) &= a_0 + a_1 t_{ij} + \ldots + a_d t_{ij}^d + \sum_{\ell=1}^L a_{d+1+\ell}(t_{ij}-t_i^{(\ell)})_+^d \\
 &= B(t_{ij})^T a,
\end{split}
$$ expressed by the scalar product of basis functions $B(t_{ij})$ and spline coefficients $a$. Defining $D_{ij} := (X_{ij}^T, B_j(t_{ij})^T)^T$ and $\theta := (\beta^T, a^T)^T$, the equation for GSMM can be rewritten as $$
\eta_{ij}^{\text{GSMM}} = D_{ij}^T\theta + Z_{ij}^Tb_i.
$$ The corresponding log-likelihood function is then expanded by a penalty term, yielding to $$
\ell^{\text{plsmmLasso}}(\beta, a, D, \phi) := \sum_{i=1}^N \log\big(p_{Y_i|b_i}(Y_i|b_i, \theta)\big) + \sum_{i=1}^N \log\big(p_{b_i}(b_i)\big) - n \sum_{p' = 1}^p P_{\lambda}(|\beta_{p'}|)
$$ with a penalty function $P_\lambda$ (e.g. SCAD) and tuning parameter $\lambda$.

Note: The second term is not necessary for the estimation of the coefficients $\beta$.

**plsmmLasso** does perform a *partial linear semiparametric mixed effects model*, but there is no further information about this specific approach.

### alqrfe

The package is based on [@danilevicz2024]. Only the random intercepts are included next to the fixed effects in this model, e.g. we assume $$
Y_{ij} = X_{ij}^T \beta + b_i + \varepsilon_{ij}.
$$ Further, this method does not model the expected value, but the $\tau$-quantile of $Y_{ij}$ given $X_{ij}$: $$
Q_{Y_{ij}}(\tau|X_{ij}) = X_{ij}^T \beta(\tau) + b_i(\tau).
$$ To express the estimation approach, we need some definitions. For $u \in \mathbb{R}$, let $\psi_\tau(u) := |\tau - 1_{u \leq 0}|$, $g(\cdot)$ a convex loss function and $\varrho_\tau(u) := \psi_\tau(u)g(u)$.

Possible choices for $g$ are: $$
\begin{split}
1) & \ g_1(u) := |u|, \\
& \ \text{quantile regression with fixed effects (QRFE)}; \\
2) & \ g_2(u) := u^2, \\
& \ \text{expectile regression with fixed effects (ERFE)}; \\
3) & \ g_3(u) := \Big(c|u| - \frac{1}{2}c^2 \Big) 1_{|u| > c} + \Big(\frac{1}{2} u^2 \Big) 1_{|u| \leq c}, \ c>0, \\ 
& \ \text{M-quantile regression with fixed effects (MQRFE)}.
\end{split}
$$ Note: $g_3$ is the so-called Huber loss function and a robust mixture of $g_1$ and $g_2$.

Taking account of a LASSO-like penalty term, weighted by a tuning parameter $\lambda$, resolves in following equation and will be used to estimate the regression coefficients:

$$
\min_{b \in \mathbb{R}^N, \\ \beta \in \mathbb{R}^p} \sum_{i=1}^N \sum_{j=1}^T \varrho_\tau(Y_{ij} - X_{ij}^T\beta - b_i) + \lambda \sum_{i=1}^N|b_i|.
$$

The **alqrfe** package calculates $\lambda$ via a grid within the main function *qr*. There, the method 'lqrfe' ('l' for LASSO) relates to above expression, but there is also the 'adaptive' variant of it, 'alqrfe', using solution of QRFE as weights for 'lqrfe'. But there is no further literature about this and it is unclear, which $g$ is used.

### glmmLasso

[@groll2012] expanded the PQL by an additional penalty term for the fixed effects, motivated by the LASSO regularization: $$
\ell^{\text{glmmLasso}}(\beta, \phi, G) := \ell^{\text{PQL}}(\beta, \phi, G) - \lambda \lVert \beta \rVert_1,
$$ where the $\text{argmax} \ \ell^{\text{glmmLasso}}(\beta, \phi, G)$ will preserve the estimates for $\beta$ and $b$.

But since there is no default setting for the penalty parameter $\lambda$, nor a default setting for the grid.

# Bayesian based methods

## Mathematical backgroud

Assume a regression model with $p$ independent predictors
$$
Y \mid X, \beta, \sigma^2 \sim \mathcal{N}(X^T\beta, \sigma^2I).
$$
We have to find the posterior distribution to $\beta$ which needs to provide a rationale to decide wether a variable $X_i$ is included in the final model or not. The two R packages *sparsereg* and *spikeSlabGAM* are based on two different approaches to model the distribution of $\beta$. The first one extens the Bayesian LASSO apporoach while the secound one applies the SSVS / spike-and-slab framework to the GAMM (generalized additive mixed model).


#### The Oracle Property

Assume a model $Y_i = X_i^T\beta + \varepsilon_i$ with centered errors $\varepsilon_i$ that have finite fourth moments and a set $S := \lbrace k \mid \beta_k \neq 0 \rbrace$ of the incices of the *in-truth nonzero* eintries of $\beta$. An *oracle estimator* $\hat{\beta}^\text{oracle}$ satisfies $$
\begin{split}
1) & \ \text{Consistent variable selection, i.e. } \lim\limits_{N \rightarrow \infty} \lbrace k \mid \hat{\beta}^\text{oracle} \neq 0 \rbrace = S \\
2) & \ \text{Optimale estimation rate, i.e. } \sqrt{N}\left(\hat{\beta}^\text{oracle}_S - \beta_S\right) \xrightarrow{\text{d}} \mathcal{N}(0_{|S|}, \Sigma^*_S), \\
& \ \text{with } \Sigma^*_S \text{ the asymptotic covariance matrix from the true subset model}
\end{split}
$$

For example, the adaptive LASSO (frequentist approach) satisfies the Oracle Property.

#### Spike & Slab regression

See [@Dab2019] and [@PN2015] for further insights of this subsection.  Set
$$
\beta_i \sim (1-\pi_i)\delta_0 + \pi_i \mathcal{N}(0, \sigma^2 \tau^2), \ i = 1,\ldots,p
$$
with $\pi_i \in [0,1]$ a mixture weight, $\sigma^2$ the error variance, $\delta_0$ the Dirac delta function with mass on $0$ (this is corresponding as the *spike*) and $\tau^2$ the variance of the so-called *slab*. In fact, the above structure is hierachical, i.e. there are additional prior assumptions:
$$
\begin{split}
\pi &\sim \text{Ber}(\theta) \\
\theta &\sim \text{Beta}(a,b) \\
\sigma^2 &\sim \Gamma^{-1}(\alpha_1, \alpha_2) \\
\tau^2 &\sim \Gamma^{-1}\left(\frac{1}{2}, \frac{s^2}{2} \right)
\end{split}
$$
To 'solve' this system, the Gibbs sampler is used. Therefore, we need the posterior distributions of the conditional hyperparameters. It turns out that $\pi \mid \beta, \tau^2, \theta$ is computationally problematic due to the Dirac function. A solution to this is to replace the Dirac function by a centered normal distribution with very less variance. This is know as

#### SSVS (stochastic search variable selection)

In this approach, a coefficient won't be set to exactally $0$, but a perteriori to a very 'small' area around $0$. To be precise, we assume
$$
\beta_i \mid \gamma_i \sim (1-\gamma_i)\mathcal{N}(0, \tau_i^2) + \gamma_i \mathcal{N}(0, \tau_i^2c_i^2)
$$
with $\mathbb{P}(\gamma_i = 1) = 1- \mathbb{P}(\gamma = 0) = \pi_i$ from the above section, $\tau_i^2$ 'small' and $\tau_i^2c_i^2$ 'large' in comparison. 

Note: It is assumed that the $\gamma_i$s are independent, i.e.
$$
\pi(\gamma) = \prod_{i=1}^p \pi_i^{\gamma_i} (1-\pi_i)^{(1-\gamma_i)}.
$$
This means that the inclusion of feature $X_{\ell}$ is independent of the inclusion of $X_{\ell'}$ for all $\ell \neq \ell'$.

The output of the SSVS is the posterior sample $\lbrace \beta^{(t)}, \sigma^{(t)}, \gamma^{(t)} \rbrace_{t=1}^T$, where $t$ indicates the iteration. The estimated inclusion probability of a feature $X_i$ can be obtained by
$$
\hat{\mathbb{P}}(\gamma_i = 1 \mid Y) = \frac{1}{T}\sum_{t=1}^T\gamma_i^{(t)}.
$$


## Identified software packages

The following R packages are based on Bayesian frameworks.

### sparsereg

The underlying Bayesian method is called *LASSOplus* and is first described in [@ratkovic2017]. The frequentist LASSO approach can be interpreted in the Bayesian framework as the MAP (maximum a perstiori) estimate of a certain model. Using a *double-exponential prior* $\mathbb{P}(\beta_j \mid \lambda) = \frac{1}{2\lambda} \exp(-\lambda | \beta_j |) =: DE(\lambda)$, the Bayesian LASSO can be written as  $$
\begin{split}
Y_i \mid X_i, \beta, \sigma^2 &\sim \mathcal{N}(X_i^T\beta, \sigma^2) \\
\beta_k \mid \lambda, \sigma^2 &\sim DE\left(\frac{\lambda}{\sigma}\right) \\
\lambda^2 &\sim \Gamma(\delta, \rho)
\end{split}
$$

Extending this model, LASSOplus can be written as hierachical prior model
$$
\begin{split}
Y_i | X_i, \beta, \sigma^2 &\sim \mathcal{N}(X_i^T\beta, \sigma^2) \\
\beta_k | \lambda, w_k, \sigma^2 &\sim DE\left(\frac{\lambda w_k}{\sigma}\right) \\
\lambda^2 |  N, K &\sim \Gamma\left(K(\sqrt{N}-1), \rho\right) \\
w_k | \gamma &\sim \text{generalized Gamma}(1,1,\gamma) \\
\gamma &\sim \exp(1)
\end{split}
$$

The LASSOplus estimate is constructed from the estimate $\beta_k$ and a thresholding (described by an *inflated variance component* $\sigma_{sp}^2$) that zeroes out sufficently small values of $|\beta_k|$. Define 
$$
V_i^k := Y_i - X_{i, -k}^T \beta_{-k}
$$
as the residuals from all effects except the $k$th. The corresponding (conditional) least square estimate is then
$$
\hat{\beta}_k ^\text{ols} := \frac{\sum_{i=1}^N X_{ik} V_i^k}{\sum_{i=1}^N X_{ik}^2},
$$
which is used to construct the LASSOplus estimate for the $k$th element:
$$
\beta_k^\text{plus} | \cdot := \beta_k I\left(\left| \hat{\beta}_k ^\text{ols} \right| \geq \frac{\lambda \sigma_{sp} w_k}{N-1}\right).
$$


This threshold ensure that LASSOplus satisfies the Oracple Property.


### spikeSlabGAM

First, start with a GAMM (generalized additive mixed model)
$$
\eta = \eta_0 + X_u\beta_u + \sum\limits_{j=1}^p f_j(x)
$$
with model terms $f_j(x) = \left(f_j(x_1), \ldots, f_j(x_n) \right)^T$, $j=1,\ldots,p$. These can be constructed with basis functions $B_j$ by
$$
\begin{split}
f_j(x) &= \sum\limits_{k = 1}^{d_j} \beta_{jk}B_{jk}(x) =: B_j \beta_j, \ \text{with} \\
\mathbb{R}^{d_j} \ni \beta_j &\overset{\text{prior}}{\sim} \text{peNMIG}(v_0, w, a_\tau, b_\tau).
\end{split}
$$
peNMIG is short for *parameter expanded mormal-mixture of inverse Gamma* and an extension to the basic idea of the SSVS (as defined above) approach using *spike-and-slab* priors. Define binary $\gamma_i$ that controls the inclusion of $\beta_i$ in the model. The NMIG prior for a scalar $\alpha$ can then be written as $$
\begin{split}
\alpha \mid \gamma, \tau^2 &\overset{\text{prior}}{\sim} \mathcal{N}(0, v^2), \ \text{with} \ v^2 = \tau^2 \gamma \\
\gamma \mid w &\overset{\text{prior}}{\sim} w1_{\gamma = 1} + (1-w)1_{\gamma = v_0}, \ \text{with very small} \ v_0>0 \\
\tau^2 &\overset{\text{prior}}{\sim} \Gamma^{-1}\left(a_\tau, b_\tau \right) \\
w  &\overset{\text{prior}}{\sim} \text{Beta}(a_w, b_w)
\end{split}
$$
Now the *spike* part corresponds to the probability of a particular coefficient to be $0$ (i.e. the variance $v^2$ is very small if $\gamma = v_0$) and vice versa the *slab* part is the prior distribution of the regression coefficients with $\gamma = 1$.

[@scheipl2011] mentioned that this NMIG prior is unsuited for the simultaneous selection of coefficient vectors. The *parameter expanded* NMIG is a solution strategy: Set $\beta_i := \alpha_i \xi_i$ with $$
\begin{split}
\alpha_i &\overset{\text{prior}}{\sim} \text{NMIG}(v_o, w, a_\tau, b_\tau) \ \text{and} \\
\xi_{ik} &\overset{\text{prior}}{\sim} \mathcal{N}(m_{ik}, 1), \ m_{ik} \sim \mathcal{U}(\lbrace -1, 1 \rbrace)
\end{split}
$$
The scalar $\alpha_i$ can be interpreted as 'importance' of the $i$th model term and the vector $\xi_i$ 'distributes' $\alpha_i$ across entries of $\beta_i$. The default settings are $a_\tau = 5,\ b_\tau = 24,\ v_0 = 2.5 \cdot 10^{-4},\ a_w = b_w = 1$. The implementation is done via a MCMC sampler for peNMIG.


