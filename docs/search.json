[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Systematic review and real life-oriented evaluation on methods for feature selection in longitudinal biomedical data",
    "section": "",
    "text": "Affiliations: 1) Preventive Cardiology and Preventive Medicine, Department of Cardiology, University Medical Center of the Johannes Gutenberg University Mainz, 55131 Mainz, Germany 2) Institute of Mathematics, Johannes Gutenberg University Mainz, 55128 Mainz, Germany 3) German Center for Cardiovascular Research (DZHK), partner site Rhine Main, 55131 Mainz, Germany 4) Clinical Epidemiology and Systems Medicine, Center for Thrombosis and Hemostasis, University Medical Center of the Johannes Gutenberg University Mainz, 55131 Mainz, Germany 5) Institute of Molecular Biology (IMB), 55131 Mainz, Germany\nThis website was created to give additional informations on the research project Systematic review and real life-oriented evaluation on methods for feature selection in longitudinal biomedical data presented at ISCB 2025 in Basel. Non of the following is peer-reviewd, published or in a submission stage."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html#mathematical-backgroud",
    "href": "index.html#mathematical-backgroud",
    "title": "Systematic review and real life-oriented evaluation on methods for feature selection in longitudinal biomedical data",
    "section": "Mathematical backgroud",
    "text": "Mathematical backgroud\nThe GEE (generalized estimating equations) have the following assumptions:\n\nindependent measurements across the clusters (individuals)\nmeasurements might correlate within clusters\n\nDetails on the following concept can be found in (Garrett M. Fitzmaurice 2011).\nThe response variables for the \\(i^{\\text{th}}\\) subject can be grouped into an \\(n_i \\times 1\\) vector \\[\nY_i = \\begin{pmatrix}\n  Y_{i1} \\\\ \\vdots \\\\ Y_{in_i}\n  \\end{pmatrix}, \\\n  i = 1, \\ldots , N\n\\] (for simplicity in longitudinal cohort data: \\(n_i = T\\) for all \\(i\\)). Associated with each response, \\(Y_{ij}\\) there is a \\(p \\times 1\\) vector of covariates \\[\nX_{ij} = \\begin{pmatrix}\n  X_{ij1} \\\\ \\vdots \\\\ X_{ijp}\n  \\end{pmatrix}, \\\n  j = 1, \\ldots , T,\n\\] which can be grouped to \\[\nX_i = \\begin{pmatrix}\n  X_{i1}^T \\\\ \\vdots \\\\ X_{iT}^T\n  \\end{pmatrix},\n  = \\begin{pmatrix}\n  X_{i11} & \\dots & X_{i1p} \\\\\n  \\vdots & \\ddots & \\vdots \\\\\n  X_{iT1} & \\dots & X_{iTp}\n  \\end{pmatrix}.\n\\]\nThe generalized (marginal) mean model for coefficients \\(\\beta = (\\beta_1, \\ldots, \\beta_p)^T\\) is \\[\n\\begin{split}\n\\mathbb{E} \\left[ Y_{ij} \\mid X_{i} \\right] = \\mathbb{E} \\left[ Y_{ij} \\mid X_{ij} \\right] &= \\mu_{ij} \\\\\ng(\\mu_{ij}) &= \\beta_0 + \\beta_1 X_{ij1} + \\ldots + \\beta_p X_{ijp} \\\\\n&= X_{ij}^T \\beta\n\\end{split}\n\\]\n(with a little abuse of the notation, ignoring the intercept \\(\\beta_0\\)).\nWe assume that there exists a known ‘variance function’ \\(v\\) and a corresponding scale parameter \\(\\phi\\) such that \\[\nV_{ij} := \\text{Var} \\left[ Y_{ij} \\mid X_{i} \\right] = \\phi v(\\mu_{ij}).\n\\]\nTo adjust for the intra-class correlation structure (e.g. within the individuals by repeated measurements), we need more flexibility of the covariance. By setting \\(A_i := \\text{diag}(V_{ij})\\) and further \\[\n\\begin{split}\n\\text{Corr}\\left[ Y_{ij}, Y_{ik} \\mid X_{i} \\right] &= \\varrho_{ijk}(\\alpha) \\\\\n\\text{Corr} \\left[Y_i \\mid X_i \\right] = R_i(\\alpha) &= (\\varrho_{ijk}(\\alpha))_{jk} :=  \n\\begin{pmatrix}\n         1 & \\alpha_{12} & \\ldots & \\alpha_{1p} \\\\\n         \\alpha_{21} & 1 & \\ldots & \\alpha_{2p} \\\\\n         \\vdots & \\vdots & \\ddots & \\vdots \\\\\n         \\alpha_{p1} & \\alpha_{p2} & \\dots & 1\n    \\end{pmatrix}, \\\\\n\\end{split}\n\\]\nwe can define the working covariance matrix \\[\n\\text{Cov} \\left[Y_i \\mid X_i \\right] = V_i(\\alpha) := A_i^{1/2} R_i(\\alpha) A_i^{1/2}.\n\\] The parameters \\(\\alpha\\) characterizes the correlation.\nNote: Since \\(\\mu_{ij} = \\mu_{ij}(\\beta)\\) is dependent on the regression coefficients, we also have \\(V_i(\\alpha) = V_i(\\alpha, \\beta)\\).\nTo estimate the regression coefficients \\(\\beta\\), we have to solve the following score equation: \\[\n\\begin{split}\nU(\\beta, \\alpha) = U(\\beta) :=& \\sum_{i=1}^N D_i^T V_i^{-1}(Y_i-\\mu_i) \\\\\n\\overset{!}{=}& \\ 0,\n\\end{split}\n\\tag{1}\\]\nwith \\(D_i := \\frac{\\partial \\mu_i}{\\partial \\beta}\\).\nNote: In the Gaussian case, i.e. \\(g(\\mu_{ij}) = \\mu_{ij}\\), we have \\(\\frac{\\partial \\mu_i}{\\partial \\beta} = X_i\\). Additional, when writing \\(g(\\mu_{ij}) =: \\eta_{ij}\\) in the general case we get \\(\\frac{\\partial \\mu_i}{\\partial \\beta} = \\frac{\\partial \\mu_i}{\\partial \\eta_{i}} X_i\\)"
  },
  {
    "objectID": "index.html#identified-software-packages",
    "href": "index.html#identified-software-packages",
    "title": "Systematic review and real life-oriented evaluation on methods for feature selection in longitudinal biomedical data",
    "section": "Identified software packages",
    "text": "Identified software packages\nThese identified R packages are based on the GEE framework.\n\nPGEE [not selected]\nPGEE does not provide a default threshold to decide if a variable is selected or not. Therefore it was excluded from the methods during screening. But since most of the following GEE-based packages relies on this method, it is usefull to present it’s approach at the beginning of this section:\nThe R package PGEE (Wang, Zhou, and Qu 2012) adds a penalty term to the score equation 1. More precisely, the SCAD (smoothly clipped absolute deviation) penalty is chosen: \\[\nq_{\\lambda}(\\theta) := \\lambda \\left(\n  1_{\\theta \\leq \\lambda} +\n  \\frac{(a \\lambda - \\theta)_+}{(a-1)\\lambda} 1_{\\theta &gt; \\lambda} \\right)\n\\] Where \\(\\theta \\geq 0\\), \\(a &gt; 2\\) (set to \\(a=3.7\\)). The penalized regression coefficients can be calculated by solving the modified equation: \\[\n\\begin{split}\nU^\\text{PGEE}(\\beta) :=& \\ U(\\beta) - q_\\lambda(|\\beta|) \\circ \\text{sign}(\\beta)  \\\\\n\\overset{!}{=}& \\ 0,\n\\end{split}\n\\tag{2}\\] where \\(\\circ\\) denotes the Hadamard product (element-wise product) of two vectors.\n\n\nLassoGEE\nNo references were found for this package. However, checking the code of the main functions revealed the direct usage of the PGEE functions, but own functions to include the \\(L^1\\)-penalty were additionally included.\n\n\npgee.mixed\nReference paper is (Deshpande, Dey, and Schifano 2019) Can be applied for multivariate outcomes and is for the univariate case the same as PGEE. There is an additional method for controlling the FDR of the selection, but this is (currently) only in the multivariate setting available.\n\n\ngeeVerse\nThe R package geeVerse (Zu et al. 2022) is based on PGEE but uses a quantile regression approach, i.e. for a given quantile \\(\\tau \\in (0,1)\\) we consider \\[\n\\theta_\\tau(Y_i \\mid X_i) = X_i \\beta_\\tau\n\\] with different regression coefficients \\(\\beta_\\tau (=: \\beta)\\) at different quantile levels. Instead of using \\(U(\\beta)\\), we have to adjust this a bit: \\[\n\\tilde{U}(\\beta) :=  \\sum_{i=1}^N X_i^T \\Gamma_i R_i^{-1}(\\tau - 1_{Y_i \\leq X_i\\beta}),\n\\] where \\(\\Gamma_i\\) is a \\(T \\times T\\) diagonal matrix of the conditional density of error \\(Y_{ij} - X_{ij}\\beta\\) and \\(\\tau - 1_{Y_i \\leq X_i\\beta}\\) is seen component wise.\nThe corresponding quantile penalized generalized estimating equation is then defined as \\[\n\\begin{split}\nU^\\text{geeVerse}(\\beta) :=& \\ \\tilde{U}(\\beta) - q_\\lambda(|\\beta|) \\circ \\text{sign}(\\beta)  \\\\\n\\overset{!}{=}& \\ 0.\n\\end{split}\n\\tag{3}\\]\n\n\nsgee\nThe package sgee (Vaughan et al. 2017) builds a model via a stagewise procedure: Start with a null model, i.e. \\(\\beta^{[0]} := 0\\) and update this to \\(\\beta^{[t]} := \\beta^{[t-1]} + \\delta^{[t]}\\), \\(t \\in \\mathbb{N}\\). In the general framework, the update increment is defined as \\[\n\\begin{split}\n\\delta^{[t]} :=& \\ \\underset{\\delta \\in \\mathbb{R}^p}{\\mathrm{arg\\min}}\n  f(\\beta^{[t-1]} + \\delta) - f(\\beta^{[t-1]}) \\\\\n  & \\text{subject to} \\ \\phi(\\beta^{[t-1]} + \\delta) - \\phi(\\beta^{[t-1]}) \\leq \\varepsilon,\n\\end{split}\n\\] where \\(\\varepsilon &gt; 0\\) is the step size and \\(\\phi\\) the penalty function (which satisfies the triangular inequality - in our case it’s the LASSO penalty \\(\\phi(\\delta) = \\lVert \\delta \\rVert_1\\)). Via Taylor expansion, one can approximate the general framework to \\[\n\\delta^{[t]} = \\underset{\\delta \\in \\mathbb{R}^p}{\\mathrm{arg\\min}}\n\\langle \\nabla f(\\beta^{[t-1]}) , \\delta \\rangle \\ \\text{subject to} \\ \\phi(\\delta) \\leq \\varepsilon.\n\\]\nTo merge the GEE approach 1 to this structure, we interpret \\(U(\\beta, \\nu) = \\nabla f(\\beta, \\nu)\\), where the nuisance parameters \\(\\nu = (\\beta_0, \\psi, \\alpha)\\) contains information about the intercept \\(\\beta_0\\), the dispersion \\(\\psi\\) and the intra-correlation structure \\(\\alpha\\). The stagewise procedure starts then also with \\(\\beta^{[0]} = 0\\) and updates to\n\\[\n\\begin{split}\ni) & \\ \\text{Given} \\ \\beta^{[t-1]}, \\ \\text{update the nuisance parameters to obtain} \\ \\nu^{[t]} \\\\\nii) & \\ \\delta^{[t]} = \\underset{\\delta \\in \\mathbb{R}^p}{\\mathrm{arg\\min}}\n      \\langle U(\\beta^{[t-1]}, \\nu^{[t]}) , \\delta \\rangle \\\n      \\text{subject to} \\ \\phi(\\delta) \\leq \\varepsilon, \\\\\niii) & \\ \\beta^{[t]} = \\beta^{[t-1]} + \\delta^{[t]}.\n\\end{split}\n\\]\nNote: In (Vaughan et al. 2017), the procedure is generalized for the bi-level setting using the group LASSO (gLASSO) penalty."
  },
  {
    "objectID": "index.html#mathematical-backgroud-1",
    "href": "index.html#mathematical-backgroud-1",
    "title": "Systematic review and real life-oriented evaluation on methods for feature selection in longitudinal biomedical data",
    "section": "Mathematical backgroud",
    "text": "Mathematical backgroud\nIn contrast to the approach via a marginal model (GEE), the mixed effects models are based on the assumption of heterogeneity across the individuals in some predictor variables. Using the same notation as above, we introduce a sub-matrix \\(Z_i \\leq X_i\\), i.e. all columns of \\(Z_i\\) are columns of \\(X_i\\). Given a collection of additional coefficients \\(b_i \\sim N(0,G)\\) and some errors \\(\\varepsilon_i \\sim N(0,\\sigma^2I_T)\\) for each subject \\(i\\) (all independent of the predictors \\(X\\)), we can write\n\\[\nY_i = X_i^T \\beta + Z_i^T b_i + \\varepsilon_i.\n\\] The regression is now formulated as\n\\[\n\\mu_{ij} := E[Y_{ij} \\mid b_i] = X_{ij}^T \\beta + Z_{ij}^T b_i,\n\\]\nwhere \\(\\beta\\) is interpreted as fixed effects and \\(b_i\\) as random effects.\nThe generalized mixed effects are therefore given by\n\\[\n\\eta_{ij} := g(E[Y_{ij} \\mid b_i]) = X_{ij}^T \\beta + Z_{ij}^T b_i.\n\\tag{4}\\]\nThe term \\(X_{ij}^T\\beta\\) can be seen as difference from the population mean and the additional term \\(Z_{ij}^Tb_i\\) accounts the subject-specific effect. Therefore, the \\(b_i\\) introduces a correlation structure (marginally) between the \\(Y_i\\).\nNote: As usual, \\((\\mu_{ij})_j\\) are independent and belong to the exponential family with \\(\\text{Var} [\\mu_{ij}] = \\phi v(\\mu_{ij})\\) for a variance function \\(v\\) and a dispersion parameter \\(\\phi\\).\nA common procedure to solve the regression formula for mixed effects models is via maximum likelihood estimation. The basis assumptions implies that \\(Y_i | b_i\\) has an exponential family distribution and \\(b_i\\) has a multivariate normal distribution with zero mean and covariance matrix \\(G\\). The (integrated) likelihood function is then defined as \\[\nL(\\beta, \\phi, G) = \\prod_{i=1} ^N\\int f(Y_i|b_i; \\phi) f(b_i; G) \\ \\text{d}b_i.\n\\] Therefore, the likelihood depends on the covariance of \\(b_i\\), but not on the unobserved \\(b_i\\). More details can be found in (Garrett M. Fitzmaurice 2011).\nAs usual, the lowercase \\(\\ell\\) denotes the log-likelihood function. Since the above integral has no closed form, a numerical solution to the (log-)likelihood function is often computed via an approximation of the integrand. This is the so-called penalized quasi-likelihood (PQL): \\[\n\\ell^{\\text{PQL}}(\\beta, \\phi, G) := \\sum_{i=1}^N \\log\\big(f(Y_i|b_i)\\big) - \\frac{1}{2}b^TGb.\n\\]"
  },
  {
    "objectID": "index.html#identified-software-packages-1",
    "href": "index.html#identified-software-packages-1",
    "title": "Systematic review and real life-oriented evaluation on methods for feature selection in longitudinal biomedical data",
    "section": "Identified software packages",
    "text": "Identified software packages\nThe following identified R packages are based on the GLMM (genealized linear mixed effects model) framework.\n\nglmmLasso\n(Groll and Tutz 2012) expanded the PQL by an additional penalty term for the fixed effects, motivated by the LASSO regularization: \\[\n\\ell^{\\text{glmmLasso}}(\\beta, \\phi, G) := \\ell^{\\text{PQL}}(\\beta, \\phi, G) - \\lambda \\lVert \\beta \\rVert_1,\n\\] where the \\(\\text{argmax} \\ \\ell^{\\text{glmmLasso}}(\\beta, \\phi, G)\\) will preserve the estimates for \\(\\beta\\) and \\(b\\).\nBut since there is no default setting for the penalty parameter \\(\\lambda\\), nor a default setting for the grid.\n\n\nrpql\n(Hui, Müller, and Welsh 2017) built up a similar framework, by subtracting additional terms from the PQL: \\[\n\\ell^{\\text{rpql}}(\\beta, \\phi, G) := \\ell^{\\text{PQL}}(\\beta, \\phi, G) - \\lambda\\sum_{p' = 1}^pv_{p'}|\\beta_{p'}| - \\lambda \\sum_{q'=1}^q w_{q'}\\lVert b_{\\bullet q'} \\rVert_2,\n\\] where \\(v_{p'}\\) and \\(w_{q'}\\) are weights analogous to the adaptive (group) LASSO (i.e. the reciprocal of the OLS) . The coefficients are then also estimated by \\(\\text{argmax} \\ \\ell^{\\text{rpql}}(\\beta, \\phi, G)\\).\n\n\nsplmm\nThe idea of the approach of (Yang and Wu 2022) is to penalize both fixed and random effects. To penalize the random effects, the correlation matrix must be considered: Rewrite \\(G = \\sigma^2 D\\) and apply the Cholesky decomposition \\(D = LL^T\\). The advantage of this decomposition is that now the \\(k\\)-th row of \\(L\\) (\\(L_{(k)}\\)) is linked to the \\(k\\)-th random effect, i.e. if \\(L_{(k)} = 0\\), the \\(k\\)-th random effect will be removed from the model. A similar approximation is done to simplify the likelihood function and in addition, the nuisance \\(\\phi\\) is substituted leading to the profile log-likelihood \\[\n\\ell^{\\text{PL}}(\\beta, D) := \\frac{1}{2} \\sum_{i=1}^N \\log|V_i| + \\frac{N}{2}\\log\\Big(\\sum_{i=1}^N (Y_i - X_i\\beta)^T V_i^{-1} (Y_i - X_i\\beta) \\Big),\n\\] with \\(V_i := Z_iDZ_i^T + I_T\\) the (standardized) conditional covariance matrix of \\(Y_i\\). Extending this with two simultaneous applied penalties leads to the final objective function: \\[\n\\ell^{\\text{splmm}}(\\beta, D) := \\ell^{\\text{PL}}(\\beta, D) + \\sum_{p'=1}^p P_{\\lambda_1}(|\\beta_{p'}|) + \\sum_{q'=2}^q P_{\\lambda_2}(\\lVert L_{(k)} \\rVert_2).\n\\] The first penalty function, \\(P_{\\lambda_1}\\) regulates the sparsity of \\(\\beta\\), while \\(P_{\\lambda_2}\\) regulates the sparsity of the random effects (via sparsity of \\(L\\)). Starting at \\(q' = 2\\) ensures to keep the random intercept in the model.\nNote: \\(P_{\\lambda_2}\\) introduces a group penalty on the rows of \\(L\\), achieving the shrinkage of all entries of a certain row to \\(0\\).\nThe splmm package currently supports the LASSO and SCAD (default setting) penalties as independent choices for \\(P_{\\lambda_k}\\).\n\n\nplsmmLasso\nTo understand this method, we first have to introduce GSMM (Generalized Semiparametric Mixed Models) (Taavoni and Arashi 2021): \\[\n\\eta_{ij}^{\\text{GSMM}} := X_{ij}^T \\beta + Z_{ij}^T b_i + f(t_{ij}),\n\\] where \\(t_{ij}\\) is the time point of the \\(j\\)-th measurement of subject \\(i\\) and \\(f\\) a continuous and twice differentiable function on some finite interval. The unspecified function \\(f\\) is approximated via \\[\n\\begin{split}\nf(t_{ij}) &= a_0 + a_1 t_{ij} + \\ldots + a_d t_{ij}^d + \\sum_{\\ell=1}^L a_{d+1+\\ell}(t_{ij}-t_i^{(\\ell)})_+^d \\\\\n&= B(t_{ij})^T a,\n\\end{split}\n\\] expressed by the scalar product of basis functions \\(B(t_{ij})\\) and spline coefficients \\(a\\). Defining \\(D_{ij} := (X_{ij}^T, B_j(t_{ij})^T)^T\\) and \\(\\theta := (\\beta^T, a^T)^T\\), the equation for GSMM can be rewritten as \\[\n\\eta_{ij}^{\\text{GSMM}} = D_{ij}^T\\theta + Z_{ij}^Tb_i.\n\\] The corresponding log-likelihood function is then expanded by a penalty term, yielding to \\[\n\\ell^{\\text{plsmmLasso}}(\\beta, a, D, \\phi) := \\sum_{i=1}^N \\log\\big(p_{Y_i|b_i}(Y_i|b_i, \\theta)\\big) + \\sum_{i=1}^N \\log\\big(p_{b_i}(b_i)\\big) - n \\sum_{p' = 1}^p P_{\\lambda}(|\\beta_{p'}|)\n\\] with a penalty function \\(P_\\lambda\\) (e.g. SCAD) and tuning parameter \\(\\lambda\\).\nNote: The second term is not necessary for the estimation of the coefficients \\(\\beta\\).\nplsmmLasso does perform a partial linear semiparametric mixed effects model, but there is no further information about this specific approach.\n\n\nalqrfe\nThe package is based on a penalized quantile regression approach (Danilevicz, Reisen, and Bondon 2024). Only the random intercepts are included next to the fixed effects in this model, e.g. we assume \\[\nY_{ij} = X_{ij}^T \\beta + b_i + \\varepsilon_{ij}.\n\\] More precisely, this approach models the \\(\\tau\\)-quantile of \\(Y_{ij}\\) given \\(X_{ij}\\): \\[\nQ_{Y_{ij}}(\\tau|X_{ij}) = X_{ij}^T \\beta(\\tau) + b_i(\\tau).\n\\] To express the estimation approach, we need some definitions. For \\(u \\in \\mathbb{R}\\), let \\(\\psi_\\tau(u) := |\\tau - 1_{u \\leq 0}|\\), \\(g(\\cdot)\\) a convex loss function and \\(\\varrho_\\tau(u) := \\psi_\\tau(u)g(u)\\).\nPossible choices for \\(g\\) are: \\[\n\\begin{split}\n1) & \\ g_1(u) := |u|, \\\\\n& \\ \\text{quantile regression with fixed effects (QRFE)}; \\\\\n2) & \\ g_2(u) := u^2, \\\\\n& \\ \\text{expectile regression with fixed effects (ERFE)}; \\\\\n3) & \\ g_3(u) := \\Big(c|u| - \\frac{1}{2}c^2 \\Big) 1_{|u| &gt; c} + \\Big(\\frac{1}{2} u^2 \\Big) 1_{|u| \\leq c}, \\ c&gt;0, \\\\\n& \\ \\text{M-quantile regression with fixed effects (MQRFE)}.\n\\end{split}\n\\] Note: \\(g_3\\) is the so-called Huber loss function and is a robust mixture of \\(g_1\\) and \\(g_2\\).\nTaking account of a LASSO-like penalty term, weighted by a tuning parameter \\(\\lambda\\), resolves in following equation and will be used to estimate the regression coefficients:\n\\[\n\\min_{b \\in \\mathbb{R}^N, \\\\ \\beta \\in \\mathbb{R}^p} \\sum_{i=1}^N \\sum_{j=1}^T \\varrho_\\tau(Y_{ij} - X_{ij}^T\\beta - b_i) + \\lambda \\sum_{i=1}^N|b_i|.\n\\]\nThe alqrfe package calculates \\(\\lambda\\) via a grid within the main function qr. There, the method ‘lqrfe’ (‘l’ for LASSO) relates to above expression, but there is also the ‘adaptive’ variant of it, ‘alqrfe’, using solution of QRFE as weights for ‘lqrfe’. But there is no further literature about this and it is unclear, which \\(g\\) is used (most likely \\(g_1\\) for LASSO).\n\n\nbuildmer\nThere is no peer-reviewed publication for this package, even though it is the most frequently used package in this review (\\(&gt; 170,000\\) all-time downloads). Only a vignette is revealing details on this method. The core idea is ‘finding the maximal feasible model & doing stepwise elimination from it’.\nThis algorithm is also available for the frameworks of Generalized Additive Models (GAM) and Mixed-Effects-Regression Trees (MERT)."
  },
  {
    "objectID": "index.html#mathematical-backgroud-2",
    "href": "index.html#mathematical-backgroud-2",
    "title": "Systematic review and real life-oriented evaluation on methods for feature selection in longitudinal biomedical data",
    "section": "Mathematical backgroud",
    "text": "Mathematical backgroud\nAssume a regression model with \\(p\\) independent predictors \\[\nY \\mid X, \\beta, \\sigma^2 \\sim \\mathcal{N}(X^T\\beta, \\sigma^2I).\n\\] We have to find the posterior distribution to \\(\\beta\\) which needs to provide a rationale to decide wether a variable \\(X_i\\) is included in the final model or not. The two R packages sparsereg and spikeSlabGAM are based on two different approaches to model the distribution of \\(\\beta\\). The first one extens the Bayesian LASSO apporoach while the secound one applies the SSVS / spike-and-slab framework to the GAMM (generalized additive mixed model).\n\nThe Oracle Property\nAssume a model \\(Y_i = X_i^T\\beta + \\varepsilon_i\\) with centered errors \\(\\varepsilon_i\\) that have finite fourth moments and a set \\(S := \\lbrace k \\mid \\beta_k \\neq 0 \\rbrace\\) of the incices of the in-truth nonzero eintries of \\(\\beta\\). An oracle estimator \\(\\hat{\\beta}^\\text{oracle}\\) satisfies \\[\n\\begin{split}\n1) & \\ \\text{Consistent variable selection, i.e. } \\lim\\limits_{N \\rightarrow \\infty} \\lbrace k \\mid \\hat{\\beta}^\\text{oracle} \\neq 0 \\rbrace = S \\\\\n2) & \\ \\text{Optimale estimation rate, i.e. } \\sqrt{N}\\left(\\hat{\\beta}^\\text{oracle}_S - \\beta_S\\right) \\xrightarrow{\\text{d}} \\mathcal{N}(0_{|S|}, \\Sigma^*_S), \\\\\n& \\ \\text{with } \\Sigma^*_S \\text{ the asymptotic covariance matrix from the true subset model}\n\\end{split}\n\\]\nFor example, the adaptive LASSO (frequentist approach) satisfies the Oracle Property.\n\n\nSpike & Slab regression\nSee (Dablander 2019) and (Perrakis and Ntzoufras 2015) for further insights of this subsection. Set \\[\n\\beta_i \\sim (1-\\pi_i)\\delta_0 + \\pi_i \\mathcal{N}(0, \\sigma^2 \\tau^2), \\ i = 1,\\ldots,p\n\\] with \\(\\pi_i \\in [0,1]\\) a mixture weight, \\(\\sigma^2\\) the error variance, \\(\\delta_0\\) the Dirac delta function with mass on \\(0\\) (this is corresponding as the spike) and \\(\\tau^2\\) the variance of the so-called slab. In fact, the above structure is hierachical, i.e. there are additional prior assumptions: \\[\n\\begin{split}\n\\pi &\\sim \\text{Ber}(\\theta) \\\\\n\\theta &\\sim \\text{Beta}(a,b) \\\\\n\\sigma^2 &\\sim \\Gamma^{-1}(\\alpha_1, \\alpha_2) \\\\\n\\tau^2 &\\sim \\Gamma^{-1}\\left(\\frac{1}{2}, \\frac{s^2}{2} \\right)\n\\end{split}\n\\] To ‘solve’ this system, the Gibbs sampler is used. Therefore, we need the posterior distributions of the conditional hyperparameters. It turns out that \\(\\pi \\mid \\beta, \\tau^2, \\theta\\) is computationally problematic due to the Dirac function. A solution to this is to replace the Dirac function by a centered normal distribution with very less variance. This is know as\n\n\nSSVS (stochastic search variable selection)\nIn this approach, a coefficient won’t be set to exactally \\(0\\), but a priori to a very ‘small’ area around \\(0\\). To be precise, we assume \\[\n\\beta_i \\mid \\gamma_i \\sim (1-\\gamma_i)\\mathcal{N}(0, \\tau_i^2) + \\gamma_i \\mathcal{N}(0, \\tau_i^2c_i^2)\n\\] with \\(\\mathbb{P}(\\gamma_i = 1) = 1- \\mathbb{P}(\\gamma = 0) = \\pi_i\\) from the above section, \\(\\tau_i^2\\) ‘small’ and \\(\\tau_i^2c_i^2\\) ‘large’ in comparison.\nNote: It is assumed that the \\(\\gamma_i\\)s are independent, i.e. \\[\n\\pi(\\gamma) = \\prod_{i=1}^p \\pi_i^{\\gamma_i} (1-\\pi_i)^{(1-\\gamma_i)}.\n\\] This means that the inclusion of feature \\(X_{\\ell}\\) is independent of the inclusion of \\(X_{\\ell'}\\) for all \\(\\ell \\neq \\ell'\\).\nThe output of the SSVS is the posterior sample \\(\\lbrace \\beta^{(t)}, \\sigma^{(t)}, \\gamma^{(t)} \\rbrace_{t=1}^T\\), where \\(t\\) indicates the iteration. The estimated inclusion probability of a feature \\(X_i\\) can be obtained by \\[\n\\hat{\\mathbb{P}}(\\gamma_i = 1 \\mid Y) = \\frac{1}{T}\\sum_{t=1}^T\\gamma_i^{(t)}.\n\\]"
  },
  {
    "objectID": "index.html#identified-software-packages-2",
    "href": "index.html#identified-software-packages-2",
    "title": "Systematic review and real life-oriented evaluation on methods for feature selection in longitudinal biomedical data",
    "section": "Identified software packages",
    "text": "Identified software packages\nThe following selected R packages are based on Bayesian frameworks.\n\nsparsereg\nThe underlying Bayesian method is called LASSOplus and is first described in (Ratkovic and Tingley 2017). The frequentist LASSO approach can be interpreted in the Bayesian framework as the MAP (maximum a perstiori) estimate of a certain model. Using a double-exponential prior \\(\\mathbb{P}(\\beta_j \\mid \\lambda) = \\frac{1}{2\\lambda} \\exp(-\\lambda | \\beta_j |) =: DE(\\lambda)\\), the Bayesian LASSO can be written as \\[\n\\begin{split}\nY_i \\mid X_i, \\beta, \\sigma^2 &\\sim \\mathcal{N}(X_i^T\\beta, \\sigma^2) \\\\\n\\beta_k \\mid \\lambda, \\sigma^2 &\\sim DE\\left(\\frac{\\lambda}{\\sigma}\\right) \\\\\n\\lambda^2 &\\sim \\Gamma(\\delta, \\rho)\n\\end{split}\n\\]\nExtending this model, LASSOplus can be written as hierachical prior model \\[\n\\begin{split}\nY_i | X_i, \\beta, \\sigma^2 &\\sim \\mathcal{N}(X_i^T\\beta, \\sigma^2) \\\\\n\\beta_k | \\lambda, w_k, \\sigma^2 &\\sim DE\\left(\\frac{\\lambda w_k}{\\sigma}\\right) \\\\\n\\lambda^2 |  N, K &\\sim \\Gamma\\left(K(\\sqrt{N}-1), \\rho\\right) \\\\\nw_k | \\gamma &\\sim \\text{generalized Gamma}(1,1,\\gamma) \\\\\n\\gamma &\\sim \\exp(1)\n\\end{split}\n\\]\nThe LASSOplus estimate is constructed from the estimate \\(\\beta_k\\) and a thresholding (described by an inflated variance component \\(\\sigma_{sp}^2\\)) that zeroes out sufficently small values of \\(|\\beta_k|\\). Define \\[\nV_i^k := Y_i - X_{i, -k}^T \\beta_{-k}\n\\] as the residuals from all effects except the \\(k\\)th. The corresponding (conditional) least square estimate is then \\[\n\\hat{\\beta}_k ^\\text{ols} := \\frac{\\sum_{i=1}^N X_{ik} V_i^k}{\\sum_{i=1}^N X_{ik}^2},\n\\] which is used to construct the LASSOplus estimate for the \\(k\\)th element: \\[\n\\beta_k^\\text{plus} | \\cdot := \\beta_k I\\left(\\left| \\hat{\\beta}_k ^\\text{ols} \\right| \\geq \\frac{\\lambda \\sigma_{sp} w_k}{N-1}\\right).\n\\]\nThis threshold ensure that LASSOplus satisfies the Oracple Property.\n\n\nspikeSlabGAM\nFirst, start with a GAMM (generalized additive mixed model) \\[\n\\eta = \\eta_0 + X_u\\beta_u + \\sum\\limits_{j=1}^p f_j(x)\n\\] with model terms \\(f_j(x) = \\left(f_j(x_1), \\ldots, f_j(x_n) \\right)^T\\), \\(j=1,\\ldots,p\\). These can be constructed with basis functions \\(B_j\\) by \\[\n\\begin{split}\nf_j(x) &= \\sum\\limits_{k = 1}^{d_j} \\beta_{jk}B_{jk}(x) =: B_j \\beta_j, \\ \\text{with} \\\\\n\\mathbb{R}^{d_j} \\ni \\beta_j &\\overset{\\text{prior}}{\\sim} \\text{peNMIG}(v_0, w, a_\\tau, b_\\tau).\n\\end{split}\n\\] peNMIG is short for parameter expanded mormal-mixture of inverse Gamma and an extension to the basic idea of the SSVS (as defined above) approach using spike-and-slab priors. Define binary \\(\\gamma_i\\) that controls the inclusion of \\(\\beta_i\\) in the model. The NMIG prior for a scalar \\(\\alpha\\) can then be written as \\[\n\\begin{split}\n\\alpha \\mid \\gamma, \\tau^2 &\\overset{\\text{prior}}{\\sim} \\mathcal{N}(0, v^2), \\ \\text{with} \\ v^2 = \\tau^2 \\gamma \\\\\n\\gamma \\mid w &\\overset{\\text{prior}}{\\sim} w1_{\\gamma = 1} + (1-w)1_{\\gamma = v_0}, \\ \\text{with very small} \\ v_0&gt;0 \\\\\n\\tau^2 &\\overset{\\text{prior}}{\\sim} \\Gamma^{-1}\\left(a_\\tau, b_\\tau \\right) \\\\\nw  &\\overset{\\text{prior}}{\\sim} \\text{Beta}(a_w, b_w)\n\\end{split}\n\\] Now the spike part corresponds to the probability of a particular coefficient to be \\(0\\) (i.e. the variance \\(v^2\\) is very small if \\(\\gamma = v_0\\)) and vice versa the slab part is the prior distribution of the regression coefficients with \\(\\gamma = 1\\).\n(Scheipl 2011) mentioned that this NMIG prior is unsuited for the simultaneous selection of coefficient vectors. The parameter expanded NMIG is a solution strategy: Set \\(\\beta_i := \\alpha_i \\xi_i\\) with \\[\n\\begin{split}\n\\alpha_i &\\overset{\\text{prior}}{\\sim} \\text{NMIG}(v_o, w, a_\\tau, b_\\tau) \\ \\text{and} \\\\\n\\xi_{ik} &\\overset{\\text{prior}}{\\sim} \\mathcal{N}(m_{ik}, 1), \\ m_{ik} \\sim \\mathcal{U}(\\lbrace -1, 1 \\rbrace)\n\\end{split}\n\\] The scalar \\(\\alpha_i\\) can be interpreted as ‘importance’ of the \\(i\\)th model term and the vector \\(\\xi_i\\) ‘distributes’ \\(\\alpha_i\\) across entries of \\(\\beta_i\\). The default settings are \\(a_\\tau = 5,\\ b_\\tau = 24,\\ v_0 = 2.5 \\cdot 10^{-4},\\ a_w = b_w = 1\\). The implementation is done via a MCMC sampler for peNMIG."
  }
]